{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸŽ¬ Lights, Camera, Recommend! A Hybrid Approach to Movie Suggestions\n",
        "  \n",
        "  This notebook provides a **concise, professional walkthrough** of building a **hybrid movie recommender system**\n",
        "  using the [MovieLens dataset](https://grouplens.org/datasets/movielens/).\n",
        "  Perform **data cleaning, merging, and exploratory data analysis (EDA)** to understand userâ€“movie interactions.\n",
        "  Build a **user-based collaborative filtering model** with **similarity shrinkage**â€”a technique inspired by\n",
        "  **industry leaders like Netflix**â€”to reduce noise when users share only a few ratings.\n",
        "  Add a **matrix factorization (SVD) fallback** for sparse situations.\n",
        "  This hybrid strategy blends neighborhood and latent-factor methods to deliver **personalized, high-quality recommendations**."
      ],
      "metadata": {
        "id": "zdQJY73hTDwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Essential Libraries"
      ],
      "metadata": {
        "id": "ASTBYzjLUV4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a8b081f"
      },
      "outputs": [],
      "source": [
        "%pip install numpy==1.26.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install surprise"
      ],
      "metadata": {
        "id": "0rlHMAslB1Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb56FYRUHQVP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the dataset"
      ],
      "metadata": {
        "id": "z85vOm7rUhYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAp1Bs7KG36Q"
      },
      "outputs": [],
      "source": [
        "# Download and unzip the dataset\n",
        "!wget http://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
        "!unzip ml-25m.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Scrubbing"
      ],
      "metadata": {
        "id": "Uc3gxICEVSlE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVBpKuNzHXN0"
      },
      "outputs": [],
      "source": [
        "movies = pd.read_csv('/content/ml-25m/movies.csv')\n",
        "ratings = pd.read_csv('/content/ml-25m/ratings.csv',nrows = 1_000_000)\n",
        "\n",
        "tags = pd.read_csv('/content/ml-25m/tags.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaL5YuwtH8wN"
      },
      "outputs": [],
      "source": [
        "movies.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMhxQCmJYOdV"
      },
      "outputs": [],
      "source": [
        "ratings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7CmmW7aZFiD"
      },
      "outputs": [],
      "source": [
        "tags.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khks3r-LYQdJ"
      },
      "outputs": [],
      "source": [
        "movies.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBKDGSIbHokQ"
      },
      "outputs": [],
      "source": [
        "ratings.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGrrm6WtZKuH"
      },
      "outputs": [],
      "source": [
        "tags.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtS80HRxYUPF"
      },
      "outputs": [],
      "source": [
        "ratings.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkAZOtUrZQq1"
      },
      "outputs": [],
      "source": [
        "movies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1vlrQ-JYYJ4"
      },
      "outputs": [],
      "source": [
        "ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1qRKz5GZSk-"
      },
      "outputs": [],
      "source": [
        "tags.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybbBSjdmZckU"
      },
      "outputs": [],
      "source": [
        "df = movies.merge(ratings , on = 'movieId')\n",
        "dfs = df.groupby('movieId').agg({'rating':['mean','count']})\n",
        "dfs.columns=['Rating','review_count']\n",
        "df1 = df.merge(dfs,on='movieId')\n",
        "df1.set_index('movieId',inplace = True)\n",
        "df1['Rating'] = df1['Rating'].round(1)\n",
        "df1.rename(columns = {'Rating':'avg_rating'},inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "lKQAo-DEVfHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhbKBoftET5I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "plt.hist(df1['review_count'], bins=30, edgecolor='black', alpha=0.7)\n",
        "\n",
        "plt.xlabel('Review Count')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Review Counts')\n",
        "\n",
        "# Set exactly 15 tick markers on x-axis\n",
        "max_value = df1['review_count'].max()\n",
        "plt.xticks(np.linspace(0, max_value, 25))  # 15 evenly spaced markers\n",
        "\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS39zrJZQG4I"
      },
      "outputs": [],
      "source": [
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io6BfxWfNvBH"
      },
      "outputs": [],
      "source": [
        "tags_df = tags.groupby(['movieId','userId'])['tag'].apply(list).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWHz-k9zNsLo"
      },
      "outputs": [],
      "source": [
        "df1_merged = df1.merge(tags_df, on=['movieId', 'userId'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LlQus8fXLSK"
      },
      "outputs": [],
      "source": [
        "df1_merged.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFJQXib9XVbp"
      },
      "outputs": [],
      "source": [
        "df1_merged[df1_merged['tag'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1_merged['tag'] = df1_merged['tag'].fillna(\"\")"
      ],
      "metadata": {
        "id": "2c7fizWnBp-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDBAqUu9G2pI"
      },
      "outputs": [],
      "source": [
        "tag_users = df1_merged[df1_merged['tag'].notnull()]['userId'].nunique()\n",
        "print(\"Users who used tags:\", tag_users,\"out of\",df1_merged['userId'].nunique())\n",
        "tag_movies = df1_merged[df1_merged['tag'].notnull()]['movieId'].nunique()\n",
        "print(\"Movies with tags:\", tag_movies,\"out of\",df1_merged['movieId'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7minrP2TyRC"
      },
      "outputs": [],
      "source": [
        "# Histogram of ratings\n",
        "df1_merged['rating'].hist(bins=10, edgecolor='black')\n",
        "plt.title(\"Ratings Distribution\")\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Avg ratings per user\n",
        "user_avg = df1_merged.groupby('userId')['rating'].mean()\n",
        "user_avg.hist(bins=30, edgecolor='black')\n",
        "plt.title(\"User Average Rating Distribution\")\n",
        "plt.xlabel(\"Avg Rating per User\")\n",
        "plt.ylabel(\"Count of Users\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd_-GVTnXj-K"
      },
      "outputs": [],
      "source": [
        "tags_only = df1_merged.dropna(subset=[\"tag\"])  # remove rows with null tags\n",
        "\n",
        "# Flatten tag list into individual tags\n",
        "all_tags = tags_only['tag'].explode()\n",
        "\n",
        "top_tags = all_tags.value_counts().head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_tags.values, y=top_tags.index)\n",
        "plt.title(\"Top 20 Tags by Frequency\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Tag\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2ShLteZakxo"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "tags_only = df1_merged.dropna(subset=[\"tag\"])\n",
        "all_tags = tags_only['tag'].explode()\n",
        "\n",
        "text = \" \".join(all_tags)\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud of Tags\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zmEycaDYGWc"
      },
      "outputs": [],
      "source": [
        "# How many unique tags per movie\n",
        "movie_tag_counts = df1_merged.groupby('movieId')['tag'].apply(lambda x: x.dropna().explode().nunique())\n",
        "movie_tag_counts.hist(bins=30, edgecolor='black')\n",
        "plt.title(\"Distribution of Tag Variety per Movie\")\n",
        "plt.xlabel(\"Unique Tags per Movie\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Which movies have the richest tags\n",
        "rich_tagged_movies = movie_tag_counts.sort_values(ascending=False).head(10)\n",
        "print(rich_tagged_movies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH7ryP03Ye9N"
      },
      "outputs": [],
      "source": [
        "# Tags per user\n",
        "user_tag_counts = df1_merged.groupby('userId')['tag'].apply(lambda x: x.dropna().explode().nunique())\n",
        "user_tag_counts.hist(bins=30, edgecolor='black')\n",
        "plt.title(\"Distribution of Tag Variety per User\")\n",
        "plt.xlabel(\"Unique Tags per User\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Top tagging users\n",
        "top_tagging_users = user_tag_counts.sort_values(ascending=False).head(10)\n",
        "print(top_tagging_users)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUjZlk9jYi3A"
      },
      "outputs": [],
      "source": [
        "# Does more tags mean more reviews?\n",
        "tag_review_corr = df1_merged.groupby('movieId').agg({\n",
        "    'tag': lambda x: x.dropna().explode().nunique(),\n",
        "    'userId': 'count'\n",
        "}).corr()\n",
        "print(tag_review_corr)\n",
        "\n",
        "# Scatter plot\n",
        "agg_df = df1_merged.groupby('movieId').agg(\n",
        "    tags_count = ('tag', lambda x: x.dropna().explode().nunique()),\n",
        "    reviews_count = ('userId','count'),\n",
        "    avg_rating = ('rating','mean')\n",
        ").reset_index()\n",
        "\n",
        "plt.scatter(agg_df['tags_count'], agg_df['avg_rating'], alpha=0.5)\n",
        "plt.title(\"Tags Count vs Avg Rating per Movie\")\n",
        "plt.xlabel(\"Unique Tags\")\n",
        "plt.ylabel(\"Avg Rating\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQATBmxSpuvm"
      },
      "outputs": [],
      "source": [
        "df1.drop(['rating','userId','timestamp'],axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVDAGLzjqOuC"
      },
      "outputs": [],
      "source": [
        "df1.drop_duplicates(inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU7IyUx0q29m"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df1['avg_rating'], bins=15, kde=True)\n",
        "plt.title(\"Distribution of Average Ratings\")\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Count of Movies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmJ0vnPRrExd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df1['review_count'], bins=50, log_scale=(False, True))  # log scale for skewness\n",
        "plt.title(\"Distribution of Review Counts\")\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Count of Movies\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tbLKUMhrged"
      },
      "outputs": [],
      "source": [
        "top_reviewed = df1.sort_values('review_count', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=top_reviewed, x='review_count', y='title', palette=\"viridis\")\n",
        "plt.title(\"Top 20 Most Reviewed Movies\")\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Movie Title\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QSeabkmteAq"
      },
      "outputs": [],
      "source": [
        "top_rated = df1[df1['review_count'] >= 500] \\\n",
        "                .sort_values('avg_rating', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=top_rated, x='avg_rating', y='title', palette=\"viridis\")\n",
        "plt.title(\"Top 20 Highest Rated Movies (min 500 reviews)\")\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Movie Title\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-4qTYVbtzMh"
      },
      "outputs": [],
      "source": [
        "top_rated = df1[df1['review_count'] >= 500].sort_values('avg_rating', ascending=False).head(20)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(data=top_rated, x='avg_rating', y='title', palette=\"coolwarm\")\n",
        "plt.title(\"Top 20 Highest Rated Movies (min 500 reviews)\")\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Movie Title\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxj8oFaCuRJi"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(data=df1, x='review_count', y='avg_rating', alpha=0.6)\n",
        "plt.title(\"Review Count vs Average Rating\")\n",
        "plt.xlabel(\"Review Count\")\n",
        "plt.ylabel(\"Average Rating\")\n",
        "plt.xscale(\"log\")  # review counts are usually very skewed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3WRVqTrw7m1"
      },
      "outputs": [],
      "source": [
        "# Step 1: Split genres into lists\n",
        "df1_exploded = df1.copy()\n",
        "df1_exploded['genres'] = df1_exploded['genres'].str.split('|')\n",
        "\n",
        "# Step 2: Explode so each genre gets its own row\n",
        "df1_exploded = df1_exploded.explode('genres')\n",
        "\n",
        "# Step 3: Group and analyze\n",
        "genre_ratings = df1_exploded.groupby('genres')['avg_rating'].mean().reset_index()\n",
        "genre_counts = df1_exploded.groupby('genres').size().reset_index(name='movie_count')\n",
        "\n",
        "# Merge both stats\n",
        "genre_stats = genre_ratings.merge(genre_counts, on='genres')\n",
        "genre_stats = genre_stats[genre_stats['movie_count'] >= 30]  # filter rare genres\n",
        "genre_stats = genre_stats.sort_values('avg_rating', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=genre_stats, x='genres', y='avg_rating', palette='viridis')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Average Rating by Genre (min 30 movies)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyvzfidWxV3m"
      },
      "outputs": [],
      "source": [
        "df1_exploded.value_counts('genres').plot(kind = 'bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¬ Hybrid Recommender System\n",
        "\n",
        "This section implements a **Hybrid Movie Recommendation Engine** that combines **Userâ€“User Collaborative Filtering** with a **Matrix Factorization (SVD) fallback**, designed for both accuracy and robustness.\n",
        "\n",
        "### 1ï¸âƒ£ Goal\n",
        "Provide personalized movie recommendations to a target user by  \n",
        "- **Primarily** leveraging patterns of similar users (neighborhood-based filtering).  \n",
        "- **Fallback** to a latent-factor model (SVD) if sufficient neighborhood information is unavailable.\n",
        "\n",
        "### 2ï¸âƒ£ Userâ€“User Similarity Layer\n",
        "* **Userâ€“Item Matrix**: Creates a pivot table of users vs. movie ratings.  \n",
        "* **Cosine Similarity**: Measures similarity between usersâ€™ rating vectors.  \n",
        "* **Minimum Overlap & Thresholding**  \n",
        "  * Considers only neighbors with **â‰¥ 5 common movies** and **similarity > 0.5**.  \n",
        "  * Ensures meaningful overlap, avoiding spurious matches when a user has rated only a few movies.\n",
        "* **Shrinkage Adjustment (Netflix-style)**  \n",
        "  * Adjusted similarity is calculated as  \n",
        "    ```\n",
        "    adjusted_sim = (overlap / (overlap + 10)) * similarity\n",
        "    ```  \n",
        "  * Reduces the effect of high similarity from very small overlapsâ€”an industry best practice.\n",
        "* **Top-Neighbor Selection**: Ranks valid neighbors by adjusted similarity and selects the top 5.\n",
        "\n",
        "### 3ï¸âƒ£ Generating Recommendations\n",
        "* From these top neighbors:\n",
        "  * Aggregates their ratings with weighted averages by adjusted similarity.\n",
        "  * Filters out movies the target user has already watched.\n",
        "  * Enhances ranking by:\n",
        "    - **Predicted score** (weighted rating)  \n",
        "    - **Genre overlap** with the userâ€™s historical preferences  \n",
        "    - **Average movie rating** across all users.\n",
        "\n",
        "### 4ï¸âƒ£ SVD Fallback\n",
        "If no neighbors satisfy the overlap/similarity criteria:\n",
        "* Applies **Singular Value Decomposition** on the full rating matrix.\n",
        "* Predicts the target userâ€™s ratings for unseen movies.\n",
        "* Selects the top 5 highest-predicted titles.\n",
        "\n",
        "### 5ï¸âƒ£ Output\n",
        "For a given `target_user_id`, the system displays:\n",
        "* Movies already watched by the user.\n",
        "* Recommended movies with predicted scores, genres, and average ratings.\n",
        "* The **method used** â€” â€œUserâ€“User Hybridâ€ or â€œSVD Fallbackâ€.\n",
        "\n",
        "### 6ï¸âƒ£ Key Advantages\n",
        "* **Robustness**: Seamless fallback ensures recommendations for cold-start or sparse users.\n",
        "* **Industry Practices**: Incorporates Netflix-style shrinkage and a hybrid design.\n",
        "* **Personalization**: Balances collaborative insights (neighborhood) with global patterns (SVD).\n"
      ],
      "metadata": {
        "id": "jeYOKr48odFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df1_merged"
      ],
      "metadata": {
        "id": "1hcgd5qvCeOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid recommender: user-user (with shrinkage & overlap) then SVD fallback\n",
        "# Dependencies: pandas, numpy, scipy, sklearn (all available in Colab)\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# -------------------------\n",
        "# 0) Prepare engine from DF (do once)\n",
        "# -------------------------\n",
        "def build_engine(df,\n",
        "                 user_col='userId', item_col='movieId', rating_col='rating',\n",
        "                 title_col='title', genres_col='genres', avg_col='avg_rating', review_col='review_count'):\n",
        "    \"\"\"\n",
        "    Build sparse rating matrix R (users x items) and metadata lookups.\n",
        "    Returns engine dict used by recommend function.\n",
        "    \"\"\"\n",
        "    # ensure stable ordering\n",
        "    users = np.sort(df[user_col].unique())\n",
        "    items = np.sort(df[item_col].unique())\n",
        "\n",
        "    user_to_idx = {u: i for i, u in enumerate(users)}\n",
        "    idx_to_user = {i: u for u, i in user_to_idx.items()}\n",
        "    item_to_idx = {it: j for j, it in enumerate(items)}\n",
        "    idx_to_item = {j: it for it, j in item_to_idx.items()}\n",
        "\n",
        "    rows = df[user_col].map(user_to_idx).to_numpy(dtype=np.int32)\n",
        "    cols = df[item_col].map(item_to_idx).to_numpy(dtype=np.int32)\n",
        "    data = df[rating_col].to_numpy(dtype=np.float32)\n",
        "\n",
        "    R = csr_matrix((data, (rows, cols)), shape=(len(users), len(items))).tocsr()\n",
        "    B = R.copy()\n",
        "    B.data = np.ones_like(B.data)  # binary interactions\n",
        "\n",
        "    # items metadata keyed by movieId\n",
        "    items_df = df[[item_col, title_col, genres_col, avg_col, review_col]].drop_duplicates(item_col).set_index(item_col)\n",
        "\n",
        "    engine = {\n",
        "        'R': R,\n",
        "        'B': B,\n",
        "        'user_to_idx': user_to_idx,\n",
        "        'idx_to_user': idx_to_user,\n",
        "        'item_to_idx': item_to_idx,\n",
        "        'idx_to_item': idx_to_item,\n",
        "        'items_df': items_df\n",
        "    }\n",
        "    return engine\n",
        "\n",
        "# -------------------------\n",
        "# 1) User-user neighbor finder\n",
        "# -------------------------\n",
        "def find_neighbors(target_user_id, engine,\n",
        "                   sim_threshold=0.5, min_common=5, shrinkage=10, top_k_candidates=200, top_neighbors=5):\n",
        "    \"\"\"\n",
        "    Find neighbors for the target user satisfying:\n",
        "      - raw cosine similarity >= sim_threshold\n",
        "      - overlap >= min_common\n",
        "    Return: list of tuples (neighbor_user_id, adjusted_similarity, overlap) sorted by adjusted_similarity desc\n",
        "    \"\"\"\n",
        "    R = engine['R']\n",
        "    B = engine['B']\n",
        "    user_to_idx = engine['user_to_idx']\n",
        "    idx_to_user = engine['idx_to_user']\n",
        "\n",
        "    if target_user_id not in user_to_idx:\n",
        "        return []\n",
        "\n",
        "    uidx = user_to_idx[target_user_id]\n",
        "    # if user has no ratings, no neighbors\n",
        "    if R.getrow(uidx).nnz == 0:\n",
        "        return []\n",
        "\n",
        "    # compute cosine of target row vs all users (fast)\n",
        "    sims = cosine_similarity(R[uidx], R).ravel()  # shape (n_users,)\n",
        "    sims[uidx] = 0.0\n",
        "\n",
        "    # compute overlaps vector: number of items both rated\n",
        "    overlap_vec = B.getrow(uidx).dot(B.T).toarray().ravel()  # shape (n_users,)\n",
        "\n",
        "    # initial candidates satisfying raw sim & overlap\n",
        "    candidate_mask = (sims >= sim_threshold) & (overlap_vec >= min_common)\n",
        "    candidate_idxs = np.where(candidate_mask)[0]\n",
        "    if candidate_idxs.size == 0:\n",
        "        return []\n",
        "\n",
        "    # restrict to top_k_candidates by raw sim to limit work\n",
        "    if candidate_idxs.size > top_k_candidates:\n",
        "        top_idx_in_candidates = np.argsort(sims[candidate_idxs])[-top_k_candidates:]\n",
        "        candidate_idxs = candidate_idxs[top_idx_in_candidates]\n",
        "\n",
        "    neighbors = []\n",
        "    for j in candidate_idxs:\n",
        "        overlap = int(overlap_vec[j])\n",
        "        sim = float(sims[j])\n",
        "        # shrinkage adjustment\n",
        "        adjusted = (overlap / (overlap + shrinkage)) * sim if overlap > 0 else 0.0\n",
        "        if adjusted > 0:\n",
        "            neighbors.append((idx_to_user[j], adjusted, overlap))\n",
        "\n",
        "    # sort by adjusted similarity\n",
        "    neighbors_sorted = sorted(neighbors, key=lambda x: x[1], reverse=True)\n",
        "    return neighbors_sorted[:top_neighbors]\n",
        "\n",
        "# -------------------------\n",
        "# 2) Recommend from neighbors (correct aggregation)\n",
        "# -------------------------\n",
        "def recommend_from_neighbors(target_user_id, engine, neighbors, top_n=5,\n",
        "                             prefer_avg_weight=0.25, prefer_genre_weight=0.15, base_weight=0.60):\n",
        "    \"\"\"\n",
        "    neighbors: list of (neighbor_user_id, adjusted_sim, overlap)\n",
        "    Returns DataFrame of top_n recommendations with metadata\n",
        "    \"\"\"\n",
        "    if not neighbors:\n",
        "        return pd.DataFrame(columns=['movieId','title','predicted_score','base_pred','avg_rating','review_count','genres'])\n",
        "\n",
        "    R = engine['R']\n",
        "    items_df = engine['items_df']\n",
        "    item_to_idx = engine['item_to_idx']\n",
        "    idx_to_item = engine['idx_to_item']\n",
        "    user_to_idx = engine['user_to_idx']\n",
        "\n",
        "    # target seen movies (indices)\n",
        "    uidx = user_to_idx[target_user_id]\n",
        "    seen_cols = set(R.getrow(uidx).indices.tolist())\n",
        "\n",
        "    # neighbor similarity mapping for quick lookup\n",
        "    neighbor_sim = {n_uid: sim for (n_uid, sim, ov) in neighbors}\n",
        "\n",
        "    # aggregate weighted sums\n",
        "    agg_num = {}   # col_idx -> numerator sum(sim * rating)\n",
        "    agg_den = {}   # col_idx -> denom sum(sim)\n",
        "    for n_uid, sim in neighbor_sim.items():\n",
        "        nidx = user_to_idx.get(n_uid)\n",
        "        if nidx is None:\n",
        "            continue\n",
        "        row = R.getrow(nidx)\n",
        "        for col, rating in zip(row.indices, row.data):\n",
        "            if col in seen_cols:\n",
        "                continue\n",
        "            agg_num[col] = agg_num.get(col, 0.0) + sim * float(rating)\n",
        "            agg_den[col] = agg_den.get(col, 0.0) + float(sim)\n",
        "\n",
        "    if not agg_num:\n",
        "        return pd.DataFrame(columns=['movieId','title','predicted_score','base_pred','avg_rating','review_count','genres'])\n",
        "\n",
        "    preds = []\n",
        "    # build target user genre set\n",
        "    watched_item_ids = [idx_to_item[c] for c in seen_cols]\n",
        "    target_genre_set = set()\n",
        "    for mid in watched_item_ids:\n",
        "        if mid in items_df.index:\n",
        "            g = items_df.at[mid, 'genres']\n",
        "            if pd.notna(g):\n",
        "                target_genre_set.update(str(g).split('|'))\n",
        "\n",
        "    # normalization for avg_rating: min-max in items_df\n",
        "    avg_ratings = items_df['avg_rating'].fillna(0.0)\n",
        "    min_avg, max_avg = avg_ratings.min(), avg_ratings.max()\n",
        "    def norm_avg(x):\n",
        "        if max_avg == min_avg:\n",
        "            return 0.0\n",
        "        return (x - min_avg) / (max_avg - min_avg)\n",
        "\n",
        "    for col_idx, num in agg_num.items():\n",
        "        den = agg_den.get(col_idx, 1e-9)\n",
        "        base_pred = num / den\n",
        "        movie_id = idx_to_item[col_idx]\n",
        "        if movie_id in items_df.index:\n",
        "            row = items_df.loc[movie_id]\n",
        "            avg = float(row['avg_rating']) if pd.notna(row['avg_rating']) else 0.0\n",
        "            rc = int(row['review_count']) if 'review_count' in items_df.columns and pd.notna(row['review_count']) else 0\n",
        "            title = row['title']\n",
        "            genres = row['genres'] if 'genres' in items_df.columns else ''\n",
        "        else:\n",
        "            avg = 0.0; rc = 0; title = str(movie_id); genres = ''\n",
        "\n",
        "        # genre similarity (Jaccard-like)\n",
        "        movie_genres = set(str(genres).split('|')) if genres and pd.notna(genres) else set()\n",
        "        if target_genre_set and movie_genres:\n",
        "            intersection = len(movie_genres & target_genre_set)\n",
        "            union = len(movie_genres | target_genre_set)\n",
        "            genre_sim = intersection / union if union > 0 else 0.0\n",
        "        else:\n",
        "            genre_sim = 0.0\n",
        "\n",
        "        final_score = (base_weight * base_pred) + (prefer_avg_weight * norm_avg(avg)) + (prefer_genre_weight * genre_sim)\n",
        "        preds.append({\n",
        "            'movie_col': col_idx,\n",
        "            'movieId': int(movie_id),\n",
        "            'title': title,\n",
        "            'base_pred': base_pred,\n",
        "            'predicted_score': final_score,\n",
        "            'avg_rating': avg,\n",
        "            'review_count': rc,\n",
        "            'genres': genres\n",
        "        })\n",
        "\n",
        "    preds_df = pd.DataFrame(preds).sort_values('predicted_score', ascending=False)\n",
        "    preds_df = preds_df.drop_duplicates(subset='movieId').head(top_n).reset_index(drop=True)\n",
        "    return preds_df[['movieId','title','predicted_score','base_pred','avg_rating','review_count','genres']]\n",
        "\n",
        "# -------------------------\n",
        "# 3) SVD fallback (centered, safe k)\n",
        "# -------------------------\n",
        "def svd_fallback(target_user_id, engine, top_n=5, k=50):\n",
        "    \"\"\"\n",
        "    Demean per-user, run svds on sparse R, reconstruct predictions for target user.\n",
        "    \"\"\"\n",
        "    R = engine['R']\n",
        "    user_to_idx = engine['user_to_idx']\n",
        "    idx_to_item = engine['idx_to_item']\n",
        "    items_df = engine['items_df']\n",
        "\n",
        "    if target_user_id not in user_to_idx:\n",
        "        return pd.DataFrame(columns=['movieId','title','predicted_score','avg_rating','genres'])\n",
        "\n",
        "    n_users, n_items = R.shape\n",
        "    k_safe = min(k, min(n_users, n_items) - 2)\n",
        "    if k_safe <= 0:\n",
        "        k_safe = 1\n",
        "\n",
        "    # compute user means on nonzero entries\n",
        "    user_sum = np.array(R.sum(axis=1)).ravel()\n",
        "    user_counts = np.diff(R.indptr)  # number of nonzeros per row\n",
        "    user_means = np.divide(user_sum, user_counts, out=np.zeros_like(user_sum), where=user_counts>0)\n",
        "\n",
        "    R_d = R.copy().astype(np.float32)\n",
        "    rows = R_d.nonzero()[0]\n",
        "    R_d.data = R_d.data - user_means[rows]  # subtract mean from nonzero entries\n",
        "\n",
        "    # truncated SVD\n",
        "    try:\n",
        "        U, s, Vt = svds(R_d, k=k_safe)\n",
        "    except Exception as e:\n",
        "        # fallback to small k\n",
        "        k_safe2 = min(20, min(n_users, n_items)-2)\n",
        "        U, s, Vt = svds(R_d, k=k_safe2)\n",
        "\n",
        "    S = np.diag(s)\n",
        "    R_hat = (U @ S) @ Vt  # users x items (dense)\n",
        "    # add means back\n",
        "    R_pred = R_hat + user_means.reshape(-1, 1)\n",
        "\n",
        "    uid = user_to_idx[target_user_id]\n",
        "    row_pred = R_pred[uid, :]\n",
        "\n",
        "    # mask seen items\n",
        "    seen_cols = set(R.getrow(uid).indices.tolist())\n",
        "    row_pred[list(seen_cols)] = -np.inf\n",
        "\n",
        "    top_idx = np.argpartition(row_pred, -top_n)[-top_n:]\n",
        "    top_idx = top_idx[np.argsort(row_pred[top_idx])[::-1]]\n",
        "\n",
        "    recs = []\n",
        "    for col in top_idx:\n",
        "        mid = int(idx_to_item[col])\n",
        "        predicted = float(row_pred[col])\n",
        "        if mid in items_df.index:\n",
        "            title = items_df.at[mid, 'title']\n",
        "            genres = items_df.at[mid, 'genres'] if 'genres' in items_df.columns else ''\n",
        "            avg = items_df.at[mid, 'avg_rating'] if 'avg_rating' in items_df.columns else np.nan\n",
        "            rc = items_df.at[mid, 'review_count'] if 'review_count' in items_df.columns else 0\n",
        "        else:\n",
        "            title = str(mid); genres=''; avg=np.nan; rc=0\n",
        "        recs.append({'movieId': mid, 'title': title, 'predicted_score': predicted, 'avg_rating': avg, 'genres': genres, 'review_count': rc})\n",
        "\n",
        "    return pd.DataFrame(recs).sort_values('predicted_score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Hybrid wrapper\n",
        "# -------------------------\n",
        "def hybrid_recommend(df, target_user_id,\n",
        "                     sim_threshold=0.5, min_common=5, shrinkage=10,\n",
        "                     top_neighbors=5, top_n=5, svd_k=50):\n",
        "    \"\"\"\n",
        "    Build engine, try user-user then fallback to svd.\n",
        "    Returns (watched_df, recs_df, method_used).\n",
        "    \"\"\"\n",
        "    engine = build_engine(df)\n",
        "    neighbors = find_neighbors(target_user_id, engine,\n",
        "                               sim_threshold=sim_threshold, min_common=min_common,\n",
        "                               shrinkage=shrinkage, top_neighbors=top_neighbors)\n",
        "\n",
        "    if neighbors:\n",
        "        recs = recommend_from_neighbors(target_user_id, engine, neighbors, top_n=top_n)\n",
        "        if not recs.empty:\n",
        "            method = 'user-user'\n",
        "            # watched movies\n",
        "            watched_ids = [engine['idx_to_item'][c] for c in engine['R'].getrow(engine['user_to_idx'][target_user_id]).indices.tolist()]\n",
        "            watched = df[df['movieId'].isin(watched_ids)][['movieId','title']].drop_duplicates().reset_index(drop=True)\n",
        "            return watched, recs, method\n",
        "\n",
        "    # fallback\n",
        "    recs = svd_fallback(target_user_id, engine, top_n=top_n, k=svd_k)\n",
        "    method = 'svd-fallback'\n",
        "    watched_ids = []\n",
        "    if target_user_id in engine['user_to_idx']:\n",
        "        watched_ids = [engine['idx_to_item'][c] for c in engine['R'].getrow(engine['user_to_idx'][target_user_id]).indices.tolist()]\n",
        "    watched = df[df['movieId'].isin(watched_ids)][['movieId','title']].drop_duplicates().reset_index(drop=True)\n",
        "    return watched, recs, method\n",
        "\n",
        "# -------------------------\n",
        "# Example usage:\n",
        "# -------------------------\n",
        "# set DF = df1_merged (your merged DataFrame)\n",
        "# watched_df, recs_df, used = hybrid_recommend(DF, target_user_id=10, sim_threshold=0.5, min_common=5, shrinkage=10, top_neighbors=5, top_n=5, svd_k=50)\n",
        "# print(\"System used:\", used)\n",
        "# display(\"Watched:\", watched_df)\n",
        "# display(\"Recommendations:\", recs_df)\n"
      ],
      "metadata": {
        "id": "_Wz6V6MxGQ3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usernum = 18\n",
        "watched_df, recs_df, used = hybrid_recommend(df1, target_user_id= usernum, sim_threshold=0.55, min_common=5, shrinkage=10, top_neighbors=5, top_n=5, svd_k=50)\n",
        "print(\"System used:\", used)\n",
        "display(f\"Movies Watched by user: {usernum}\", watched_df.head(10))\n",
        "display(\"Recommendations:\", recs_df)"
      ],
      "metadata": {
        "id": "0ORHSXwCFBA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "usernum = 18\n",
        "watched_df, recs_df, used = hybrid_recommend(\n",
        "    df1,\n",
        "    target_user_id=usernum,\n",
        "    sim_threshold=0.55,\n",
        "    min_common=5,\n",
        "    shrinkage=10,\n",
        "    top_neighbors=5,\n",
        "    top_n=5,\n",
        "    svd_k=50\n",
        ")\n",
        "\n",
        "# âœ… Round selected numeric columns to 1 decimal place for display\n",
        "numeric_cols = ['predicted_score', 'base_pred', 'avg_rating', 'review_count']\n",
        "for col in numeric_cols:\n",
        "    if col in recs_df.columns:\n",
        "        recs_df[col] = recs_df[col].round(1)\n",
        "\n",
        "# Title block\n",
        "print(f\"ðŸŽ¬ **Hybrid Recommendation Results for User {usernum}**\")\n",
        "print(f\"ðŸ”Ž Recommendation Engine Used: **{used}**\\n\")\n",
        "\n",
        "# Movies Watched\n",
        "display(HTML(\"<h4>ðŸ“½ Movies Already Watched (Random 10)</h4>\"))\n",
        "display(\n",
        "    watched_df.sample(10)\n",
        "    .style.set_table_styles(\n",
        "        [{'selector': 'th',\n",
        "          'props': [('background-color', '#4CAF50'), ('color', 'white')]}]\n",
        "    )\n",
        "    .hide(axis='index')\n",
        ")\n",
        "\n",
        "# Recommended Movies with nicely formatted decimals\n",
        "display(HTML(\"<h4>âœ¨ Top 5 Recommended Movies</h4>\"))\n",
        "display(\n",
        "    recs_df\n",
        "    .style.format({col: \"{:.1f}\".format for col in recs_df.select_dtypes(float).columns})\n",
        "    .set_table_styles(\n",
        "        [{'selector': 'th',\n",
        "          'props': [('background-color', '#FF9800'), ('color', 'white')]}]\n",
        "    )\n",
        "    .hide(axis='index')\n",
        ")\n"
      ],
      "metadata": {
        "id": "mElXNgVbpLAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In this project :\n",
        "\n",
        "*   Cleaned and merged MovieLens data and explored key trends.  \n",
        "*   Built a hybrid recommender with **userâ€“user filtering + shrinkage** and an **SVD fallback**.\n",
        "\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "*   Shrinkage improved reliability when user overlap was low, reflecting Netflixâ€™s own practices.\n",
        "*   The hybrid design produced stable recommendations even for sparse users.\n",
        "This demonstrates how combining neighborhood-based and latent factor approaches yields a scalable, accurate recommender.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q651N4VaWBbP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}